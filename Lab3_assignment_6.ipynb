{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJLmn+zmCKkFBX2pVVm+77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amulyanrao7777/NLP/blob/main/Lab3_assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[QUESTION-Lab3[link text](https://)](https://docs.google.com/document/d/1TymWXo_sF89LY6dvzHXGuMKVdYllDdvvbE0dLWM0CoE/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "0XMsyIsVK05g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1bd876",
        "outputId": "d661d3d7-6764-4158-8e4e-59ebf65a9f01"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('gutenberg', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "print(\"NLTK data downloaded.\")\n",
        "\n",
        "# Load the raw text of a suitable book from the Gutenberg corpus\n",
        "corpus_text = gutenberg.raw('austen-emma.txt')\n",
        "print(f\"Loaded text from: {gutenberg.fileids()[0]}\")\n",
        "print(f\"First 500 characters of the corpus:\\n{corpus_text[:500]}...\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n",
            "NLTK data downloaded.\n",
            "Loaded text from: austen-emma.txt\n",
            "First 500 characters of the corpus:\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "543b5c89",
        "outputId": "94671dbb-73ed-4df0-f370-738ee3f7904f"
      },
      "source": [
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "def build_vocab(text, min_freq):\n",
        "    # a. Tokenize the input text into words.\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # b. Count the frequency of each word.\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # c. Create a set of unique words (your vocabulary) that appear at least `min_freq` times.\n",
        "    # Include special tokens <s>, </s>, and <UNK> in this vocabulary.\n",
        "    vocab = {'<s>', '</s>', '<UNK>'}\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab.add(word)\n",
        "\n",
        "    # d. Return the vocabulary set.\n",
        "    return vocab\n",
        "\n",
        "def preprocess_sentences(text, vocab):\n",
        "    processed_sentences = []\n",
        "\n",
        "    # a. Tokenize the raw text into individual sentences.\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # b. For each sentence, tokenize it into words.\n",
        "        # Lowercase words for consistency with vocabulary.\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "        # c. For each word in the sentence, check if it exists in the vocabulary set.\n",
        "        # If not, replace it with the <UNK> token.\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            if token in vocab:\n",
        "                processed_tokens.append(token)\n",
        "            else:\n",
        "                processed_tokens.append('<UNK>')\n",
        "\n",
        "        # d. Prepend each processed sentence with the <s> token and append it with the </s> token.\n",
        "        processed_tokens.insert(0, '<s>')\n",
        "        processed_tokens.append('</s>')\n",
        "\n",
        "        processed_sentences.append(processed_tokens)\n",
        "\n",
        "    # e. Return a list of these processed and tokenized sentences.\n",
        "    return processed_sentences\n",
        "\n",
        "# 6. Apply the `build_vocab` function to your loaded corpus text with a chosen `min_freq` (e.g., 5).\n",
        "min_frequency = 5\n",
        "vocabulary = build_vocab(corpus_text, min_frequency)\n",
        "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
        "print(f\"First 10 words in vocabulary: {list(vocabulary)[:10]}\")\n",
        "\n",
        "# 7. Apply the `preprocess_sentences` function using the corpus text and the vocabulary\n",
        "# obtained in the previous step to get a list of prepared sentences.\n",
        "prepared_sentences = preprocess_sentences(corpus_text, vocabulary)\n",
        "print(f\"Number of prepared sentences: {len(prepared_sentences)}\")\n",
        "print(f\"First prepared sentence: {prepared_sentences[0]}\")\n",
        "print(f\"Second prepared sentence: {prepared_sentences[1]}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 2338\n",
            "First 10 words in vocabulary: ['height', 'farm', 'age', 'falling', 'deal', 'expose', 'visited', 'your', 'kindly', 'assurances']\n",
            "Number of prepared sentences: 7493\n",
            "First prepared sentence: ['<s>', '<UNK>', 'emma', 'by', 'jane', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', '<UNK>', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', '<UNK>', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', '<UNK>', 'her', '.', '</s>']\n",
            "Second prepared sentence: ['<s>', 'she', 'was', 'the', '<UNK>', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', '<UNK>', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "177ee3f3"
      },
      "source": [
        "## N-Gram Generation\n",
        "\n",
        "### Subtask:\n",
        "Write a function to generate N-grams from a list of tokens and count their occurrences, storing them in a dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "230ce77b",
        "outputId": "4b6d7645-82ce-45c9-bf21-1055b2d596de"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def generate_ngrams(sentences, n):\n",
        "    ngram_counts = Counter()\n",
        "    for sentence in sentences:\n",
        "        # Generate n-grams for the current sentence\n",
        "        # An n-gram at position i is a tuple of n consecutive tokens starting from sentence[i]\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i : i + n])\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "# Generate bigrams (n=2) from the prepared_sentences\n",
        "bigram_counts = generate_ngrams(prepared_sentences, 2)\n",
        "\n",
        "print(f\"Total number of unique bigrams: {len(bigram_counts)}\")\n",
        "print(\"\\nTop 10 most common bigrams:\")\n",
        "for bigram, count in bigram_counts.most_common(10):\n",
        "    print(f\"'{' '.join(bigram)}': {count}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique bigrams: 48102\n",
            "\n",
            "Top 10 most common bigrams:\n",
            "'. </s>': 5149\n",
            "', and': 1882\n",
            "''' </s>': 1567\n",
            "'<s> ``': 1378\n",
            "'<UNK> ,': 1326\n",
            "'. ''': 1158\n",
            "'; and': 867\n",
            "'the <UNK>': 835\n",
            "'<UNK> --': 736\n",
            "'<UNK> .': 677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7352677"
      },
      "source": [
        "## Unsmoothed Probability (PMLE)\n",
        "\n",
        "### Subtask:\n",
        "Implement the `NGramModel` class to calculate Maximum Likelihood Estimation (MLE) probabilities for words given their history, and test it with example phrases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7be77a7b",
        "outputId": "5f301e8c-e9fa-4f7d-d49a-a3521a99c51e"
      },
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, n_gram_counts, n_minus_1_gram_counts):\n",
        "        self.n_gram_counts = n_gram_counts\n",
        "        self.n_minus_1_gram_counts = n_minus_1_gram_counts\n",
        "\n",
        "    def _get_ngram_count(self, ngram):\n",
        "        return self.n_gram_counts.get(ngram, 0)\n",
        "\n",
        "    def _get_n_minus_1_gram_count(self, history):\n",
        "        return self.n_minus_1_gram_counts.get(history, 0)\n",
        "\n",
        "    def pmle_probability(self, word, history):\n",
        "        # For PMLE, history + word forms the n-gram\n",
        "        ngram = history + (word,)\n",
        "\n",
        "        ngram_count = self._get_ngram_count(ngram)\n",
        "        history_count = self._get_n_minus_1_gram_count(history)\n",
        "\n",
        "        if history_count == 0:\n",
        "            # If history count is 0, probability is 0 (or undefined, handling as 0 here)\n",
        "            return 0.0\n",
        "\n",
        "        return ngram_count / history_count\n",
        "\n",
        "# Generate unigram counts (n-1 grams for bigrams)\n",
        "unigram_counts = Counter()\n",
        "for sentence in prepared_sentences:\n",
        "    for word in sentence:\n",
        "        unigram_counts[(word,)] += 1\n",
        "\n",
        "print(f\"Total number of unique unigrams: {len(unigram_counts)}\")\n",
        "print(\"\\nTop 10 most common unigrams:\")\n",
        "for unigram, count in unigram_counts.most_common(10):\n",
        "    print(f\"'{' '.join(unigram)}': {count}\")\n",
        "\n",
        "# Instantiate the NGramModel for bigrams\n",
        "# n_gram_counts = bigram_counts\n",
        "# n_minus_1_gram_counts = unigram_counts\n",
        "bigram_model = NGramModel(bigram_counts, unigram_counts)\n",
        "\n",
        "# Test the pmle_probability method\n",
        "print(\"\\nTesting PMLE probabilities:\")\n",
        "\n",
        "# P('the' | '<s>')\n",
        "history_1 = ('<s>',)\n",
        "word_1 = 'the'\n",
        "prob_1 = bigram_model.pmle_probability(word_1, history_1)\n",
        "print(f\"P('{word_1}' | {' '.join(history_1)}): {prob_1:.6f}\")\n",
        "\n",
        "# P('world' | 'in the')\n",
        "history_2 = ('in', 'the')\n",
        "word_2 = 'world'\n",
        "prob_2 = bigram_model.pmle_probability(word_2, history_2)\n",
        "print(f\"P('{word_2}' | {' '.join(history_2)}): {prob_2:.6f}\")\n",
        "\n",
        "# P('emma' | '<s>')\n",
        "history_3 = ('<s>',)\n",
        "word_3 = 'emma'\n",
        "prob_3 = bigram_model.pmle_probability(word_3, history_3)\n",
        "print(f\"P('{word_3}' | {' '.join(history_3)}): {prob_3:.6f}\")\n",
        "\n",
        "# P('<UNK>' | 'a')\n",
        "history_4 = ('a',)\n",
        "word_4 = '<UNK>'\n",
        "prob_4 = bigram_model.pmle_probability(word_4, history_4)\n",
        "print(f\"P('{word_4}' | {' '.join(history_4)}): {prob_4:.6f}\")\n",
        "\n",
        "# P('nonexistent' | 'word') (should be 0)\n",
        "history_5 = ('word',)\n",
        "word_5 = 'nonexistent'\n",
        "prob_5 = bigram_model.pmle_probability(word_5, history_5)\n",
        "print(f\"P('{word_5}' | {' '.join(history_5)}): {prob_5:.6f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique unigrams: 2338\n",
            "\n",
            "Top 10 most common unigrams:\n",
            "',': 12016\n",
            "'<UNK>': 9246\n",
            "'<s>': 7493\n",
            "'</s>': 7493\n",
            "'.': 6357\n",
            "'the': 5201\n",
            "'to': 5181\n",
            "'and': 4877\n",
            "'of': 4284\n",
            "'i': 3177\n",
            "\n",
            "Testing PMLE probabilities:\n",
            "P('the' | <s>): 0.038836\n",
            "P('world' | in the): 0.000000\n",
            "P('emma' | <s>): 0.026958\n",
            "P('<UNK>' | a): 0.139245\n",
            "P('nonexistent' | word): 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4a55b1"
      },
      "source": [
        "## Laplace Smoothing (Add-1)\n",
        "\n",
        "### Subtask:\n",
        "Implement Laplace (Add-1) smoothing within the `NGramModel` to address the zero-probability problem by adjusting N-gram counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a5940e3",
        "outputId": "266c7875-7a8e-4916-d2f3-426c67d5535c"
      },
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, n_gram_counts, n_minus_1_gram_counts, vocabulary_size):\n",
        "        self.n_gram_counts = n_gram_counts\n",
        "        self.n_minus_1_gram_counts = n_minus_1_gram_counts\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "    def _get_ngram_count(self, ngram):\n",
        "        return self.n_gram_counts.get(ngram, 0)\n",
        "\n",
        "    def _get_n_minus_1_gram_count(self, history):\n",
        "        return self.n_minus_1_gram_counts.get(history, 0)\n",
        "\n",
        "    def pmle_probability(self, word, history):\n",
        "        # For PMLE, history + word forms the n-gram\n",
        "        ngram = history + (word,)\n",
        "\n",
        "        ngram_count = self._get_ngram_count(ngram)\n",
        "        history_count = self._get_n_minus_1_gram_count(history)\n",
        "\n",
        "        if history_count == 0:\n",
        "            # If history count is 0, probability is 0 (or undefined, handling as 0 here)\n",
        "            return 0.0\n",
        "\n",
        "        return ngram_count / history_count\n",
        "\n",
        "    def laplace_probability(self, word, history):\n",
        "        # For Laplace smoothing, history + word forms the n-gram\n",
        "        ngram = history + (word,)\n",
        "\n",
        "        ngram_count = self._get_ngram_count(ngram)\n",
        "        history_count = self._get_n_minus_1_gram_count(history)\n",
        "\n",
        "        # Add-1 smoothing formula\n",
        "        # (count(history, word) + 1) / (count(history) + vocabulary_size)\n",
        "        numerator = ngram_count + 1\n",
        "        denominator = history_count + self.vocabulary_size\n",
        "\n",
        "        if denominator == 0:\n",
        "            # This case should ideally not happen if vocabulary_size > 0\n",
        "            return 0.0\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "# Calculate vocabulary size\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(f\"Vocabulary size for Laplace smoothing: {vocabulary_size}\")\n",
        "\n",
        "# Re-instantiate the NGramModel for bigrams with vocabulary_size\n",
        "bigram_model_smoothed = NGramModel(bigram_counts, unigram_counts, vocabulary_size)\n",
        "\n",
        "# Test the laplace_probability method\n",
        "print(\"\\nTesting Laplace probabilities:\")\n",
        "\n",
        "# P('the' | '<s>')\n",
        "history_1 = ('<s>',)\n",
        "word_1 = 'the'\n",
        "prob_1_lp = bigram_model_smoothed.laplace_probability(word_1, history_1)\n",
        "print(f\"Laplace P('{word_1}' | {' '.join(history_1)}): {prob_1_lp:.6f}\")\n",
        "\n",
        "# P('world' | 'in the') - This was 0 with PMLE\n",
        "history_2 = ('in', 'the')\n",
        "word_2 = 'world'\n",
        "prob_2_lp = bigram_model_smoothed.laplace_probability(word_2, history_2)\n",
        "print(f\"Laplace P('{word_2}' | {' '.join(history_2)}): {prob_2_lp:.6f}\")\n",
        "\n",
        "# P('emma' | '<s>')\n",
        "history_3 = ('<s>',)\n",
        "word_3 = 'emma'\n",
        "prob_3_lp = bigram_model_smoothed.laplace_probability(word_3, history_3)\n",
        "print(f\"Laplace P('{word_3}' | {' '.join(history_3)}): {prob_3_lp:.6f}\")\n",
        "\n",
        "# P('<UNK>' | 'a')\n",
        "history_4 = ('a',)\n",
        "word_4 = '<UNK>'\n",
        "prob_4_lp = bigram_model_smoothed.laplace_probability(word_4, history_4)\n",
        "print(f\"Laplace P('{word_4}' | {' '.join(history_4)}): {prob_4_lp:.6f}\")\n",
        "\n",
        "# P('nonexistent' | 'word') (should now be non-zero due to smoothing)\n",
        "history_5 = ('word',)\n",
        "word_5 = 'nonexistent'\n",
        "prob_5_lp = bigram_model_smoothed.laplace_probability(word_5, history_5)\n",
        "print(f\"Laplace P('{word_5}' | {' '.join(history_5)}): {prob_5_lp:.6f}\")\n",
        "\n",
        "# Check a history that didn't exist before in n-1 counts (e.g., ('unknown',))\n",
        "history_6 = ('unknown',)\n",
        "word_6 = 'word'\n",
        "prob_6_lp = bigram_model_smoothed.laplace_probability(word_6, history_6)\n",
        "print(f\"Laplace P('{word_6}' | {' '.join(history_6)}): {prob_6_lp:.6f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size for Laplace smoothing: 2338\n",
            "\n",
            "Testing Laplace probabilities:\n",
            "Laplace P('the' | <s>): 0.029702\n",
            "Laplace P('world' | in the): 0.000428\n",
            "Laplace P('emma' | <s>): 0.020649\n",
            "Laplace P('<UNK>' | a): 0.079824\n",
            "Laplace P('nonexistent' | word): 0.000412\n",
            "Laplace P('word' | unknown): 0.000428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4e79980"
      },
      "source": [
        "# Task\n",
        "Implement the `perplexity` function to evaluate a language model by calculating the perplexity of a given list of sentences using the smoothed bigram probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac799317"
      },
      "source": [
        "## Perplexity Calculation\n",
        "\n",
        "### Subtask:\n",
        "Implement the perplexity metric to evaluate the language model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8170e70c",
        "outputId": "ab6c7fef-34d4-44e8-a665-f406b10c3123"
      },
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(sentences, model):\n",
        "    total_log_prob = 0.0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Start from the second word (index 1) to form bigrams (history = sentence[i-1], current_word = sentence[i])\n",
        "        # The first token '<s>' does not have a preceding word within the sentence context\n",
        "        for i in range(1, len(sentence)):\n",
        "            current_word = sentence[i]\n",
        "            history = (sentence[i-1],) # History for bigram is the single preceding word\n",
        "\n",
        "            # Calculate Laplace smoothed probability\n",
        "            prob = model.laplace_probability(current_word, history)\n",
        "\n",
        "            # Avoid log(0) errors for probabilities that are effectively zero (though Laplace smoothing should prevent true zeros)\n",
        "            if prob > 0:\n",
        "                total_log_prob += math.log2(prob)\n",
        "            else:\n",
        "                # In theory, with Laplace smoothing, prob should never be exactly 0, but as a safeguard:\n",
        "                # If it somehow is 0, it means an infinitely high perplexity for that word, which would break the calculation.\n",
        "                # For practical purposes in language modeling, a very small epsilon can be used or a high penalty.\n",
        "                # For now, we will handle it by just not adding to total_log_prob if prob is 0. This might lead to unexpected results if it happens often.\n",
        "                pass\n",
        "            total_words += 1\n",
        "\n",
        "    # Handle cases where total_words might be zero to prevent division by zero\n",
        "    if total_words == 0:\n",
        "        return float('inf') # Perplexity is infinite if no words are processed\n",
        "\n",
        "    average_log_prob = total_log_prob / total_words\n",
        "    perplexity = 2 ** (-average_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "# Call calculate_perplexity() with test_sentences and bigram_model_smoothed\n",
        "# Using `test_sentences` from the notebook's kernel state\n",
        "perplexity_value = calculate_perplexity(test_sentences, bigram_model_smoothed)\n",
        "print(f\"Perplexity of the test set: {perplexity_value:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of the test set: 159.5785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5756f71"
      },
      "source": [
        "# Task\n",
        "The user wants to implement an N-gram language model. This involves:\n",
        "\n",
        "1.  Generating 5 random sentences using a Smoothed Bigram Model.\n",
        "2.  Comparing the grammatical correctness of sentences generated by Unigram and Bigram models, and explaining the differences.\n",
        "3.  Loading and preprocessing a second NLTK corpus, then training a new Smoothed Bigram Model on it.\n",
        "4.  Generating 3 sample sentences from this new model and comparing them with sentences from the initial model.\n",
        "5.  Performing a cross-domain perplexity experiment by testing each model on the other corpus's test set and explaining the results.\n",
        "6.  Reflecting on why Trigram models (N=3) face greater sparsity issues compared to Bigram models (N=2).\n",
        "7.  Finally, providing a comprehensive summary of all findings, including the generated sentences, same-domain and cross-domain perplexity scores, and the sparsity reflection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81d3186"
      },
      "source": [
        "## Sentence Generator\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to generate 5 random sentences using the Smoothed Bigram Model. The generation should start with '< s >' and continue until         \n",
        "'< /s >' or a length limit is reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c72966f",
        "outputId": "d405b7a8-1580-4be5-c7e9-451876dea2bd"
      },
      "source": [
        "import random\n",
        "\n",
        "def generate_sentence(model, vocabulary, max_length=20):\n",
        "    current_sentence = ['<s>']\n",
        "    next_words_candidates = list(vocabulary) # Convert set to list for indexing\n",
        "\n",
        "    for _ in range(max_length - 1):\n",
        "        last_word = current_sentence[-1]\n",
        "        probabilities = []\n",
        "\n",
        "        # Calculate Laplace probabilities for each potential next word\n",
        "        for next_word in next_words_candidates:\n",
        "            prob = model.laplace_probability(next_word, (last_word,))\n",
        "            probabilities.append(prob)\n",
        "\n",
        "        # Sum of probabilities for normalization\n",
        "        total_prob = sum(probabilities)\n",
        "\n",
        "        # Handle cases where total_prob might be 0 (should be rare with Laplace)\n",
        "        if total_prob == 0:\n",
        "            # If no probable next words, append <UNK> or break. Breaking here.\n",
        "            current_sentence.append('<UNK>') # Or handle as a different end condition\n",
        "            break\n",
        "\n",
        "        # Normalize probabilities to sum to 1\n",
        "        normalized_probabilities = [p / total_prob for p in probabilities]\n",
        "\n",
        "        # Randomly select the next word based on the normalized probabilities\n",
        "        next_word = random.choices(next_words_candidates, weights=normalized_probabilities, k=1)[0]\n",
        "\n",
        "        current_sentence.append(next_word)\n",
        "\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "\n",
        "    # If the sentence didn't end with </s> and reached max_length, ensure it ends.\n",
        "    if current_sentence[-1] != '</s>' and len(current_sentence) >= max_length:\n",
        "        current_sentence.append('</s>')\n",
        "\n",
        "    return current_sentence\n",
        "\n",
        "print(\"Generating 5 random sentences:\")\n",
        "for i in range(5):\n",
        "    generated_sentence = generate_sentence(bigram_model_smoothed, vocabulary)\n",
        "    # Optionally remove <s> and </s> for cleaner display if not needed for further processing\n",
        "    # display_sentence = generated_sentence[1:-1] if generated_sentence[0] == '<s>' and generated_sentence[-1] == '</s>' else generated_sentence\n",
        "    # print(f\"Sentence {i+1}: {' '.join(display_sentence)}\")\n",
        "    print(f\"Sentence {i+1}: {' '.join(generated_sentence)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 5 random sentences:\n",
            "Sentence 1: <s> she must pointed animated particulars admire depend father pass belonged able ill. rest invitations serious introducing couple convince catch </s>\n",
            "Sentence 2: <s> be accomplished wanting slow nice goodness where large determination exactly yards small exquisite walk useful madam evil comes hints </s>\n",
            "Sentence 3: <s> emma george suspicions so , you will fresh received earth public reach clear particularly willing important next passage accomplished </s>\n",
            "Sentence 4: <s> ill. table satisfaction immediate allow days support woman ordered admit relief disparity suspect faith unfortunately less height possibility direct </s>\n",
            "Sentence 5: <s> `` rain letter admiration standing between stopping state look thinking at sweet campbell admit drew occupied comfortable differently houses </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17079325"
      },
      "source": [
        "## Unigram vs. Bigram Analysis\n",
        "\n",
        "### Subtask:\n",
        "Compare the grammatical correctness of sentences generated by a Unigram model versus a Bigram model. This involves generating sentences from a Unigram model and then explaining the differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52d0b951"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing a function to generate sentences using a Unigram model. This involves defining the `generate_unigram_sentence` function, calculating unigram probabilities with Laplace smoothing, and then generating five sample sentences using this function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96e8b0fe",
        "outputId": "1eafef4a-8372-4a3a-8322-892ce3f8f0ae"
      },
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "def generate_unigram_sentence(unigram_counts, vocabulary_size, max_length=20):\n",
        "    current_sentence = ['<s>']\n",
        "\n",
        "    # Calculate total count of all unigrams for the denominator in probability calculation\n",
        "    total_unigram_count = sum(unigram_counts.values())\n",
        "\n",
        "    # Get all words in the vocabulary (excluding special tokens for generation choice,\n",
        "    # but including them if they are chosen as next word)\n",
        "    # We need to consider all words in the vocabulary for the choices\n",
        "    all_vocab_words = list(vocabulary) # Using the global vocabulary variable\n",
        "\n",
        "    for _ in range(max_length - 1):\n",
        "        probabilities = []\n",
        "\n",
        "        # Calculate Laplace-smoothed unigram probabilities for each word in the vocabulary\n",
        "        for word_candidate in all_vocab_words:\n",
        "            # Unigram count for the word_candidate\n",
        "            count_word = unigram_counts.get((word_candidate,), 0)\n",
        "\n",
        "            # Laplace-smoothed unigram probability\n",
        "            prob = (count_word + 1) / (total_unigram_count + vocabulary_size)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "        # Sum of probabilities for normalization\n",
        "        total_prob = sum(probabilities)\n",
        "\n",
        "        # If total_prob is 0 (should not happen with Laplace but as a safeguard), break\n",
        "        if total_prob == 0:\n",
        "            current_sentence.append('<UNK>') # Append UNK if no choices are available\n",
        "            break\n",
        "\n",
        "        # Normalize probabilities to sum to 1\n",
        "        normalized_probabilities = [p / total_prob for p in probabilities]\n",
        "\n",
        "        # Randomly select the next word based on the normalized probabilities\n",
        "        next_word = random.choices(all_vocab_words, weights=normalized_probabilities, k=1)[0]\n",
        "        current_sentence.append(next_word)\n",
        "\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "\n",
        "    # If the sentence didn't end with </s> and reached max_length, ensure it ends.\n",
        "    if current_sentence[-1] != '</s>' and len(current_sentence) >= max_length:\n",
        "        current_sentence.append('</s>')\n",
        "\n",
        "    return current_sentence\n",
        "\n",
        "print(\"Generating 5 random sentences using Unigram model:\")\n",
        "for i in range(5):\n",
        "    generated_s_unigram = generate_unigram_sentence(unigram_counts, vocabulary_size)\n",
        "    print(f\"Sentence {i+1}: {' '.join(generated_s_unigram)}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 5 random sentences using Unigram model:\n",
            "Sentence 1: <s> are </s>\n",
            "Sentence 2: <s> deal to he </s>\n",
            "Sentence 3: <s> the he that might good seen indeed kind -- compliment ; character woodhouse 's must dark <s> slowly <UNK> </s>\n",
            "Sentence 4: <s> it one a the any you was mr. ? evening -- and . and to and herself many . </s>\n",
            "Sentence 5: <s> ! <UNK> at information <s> more sorry knightley harriet fairfax saw situation which ; having . state she knew </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d128df19"
      },
      "source": [
        "### Comparison of Unigram vs. Bigram Generated Sentences\n",
        "\n",
        "**Bigram Generated Sentences (from previous output):**\n",
        "1. `<s> she must pointed animated particulars admire depend father pass belonged able ill. rest invitations serious introducing couple convince catch </s>`\n",
        "2. `<s> be accomplished wanting slow nice goodness where large determination exactly yards small exquisite walk useful madam evil comes hints </s>`\n",
        "3. `<s> emma george suspicions so , you will fresh received earth public reach clear particularly willing important next passage accomplished </s>`\n",
        "4. `<s> ill. table satisfaction immediate allow days support woman ordered admit relief disparity suspect faith unfortunately less height possibility direct </s>`\n",
        "5. `<s> `` rain letter admiration standing between stopping state look thinking at sweet campbell admit drew occupied comfortable differently houses </s>`\n",
        "\n",
        "**Unigram Generated Sentences:**\n",
        "1. `<s> are </s>`\n",
        "2. `<s> deal to he </s>`\n",
        "3. `<s> the he that might good seen indeed kind -- compliment ; character woodhouse 's must dark <s> slowly <UNK> </s>`\n",
        "4. `<s> it one a the any you was mr. ? evening -- and . and to and herself many . </s>`\n",
        "5. `<s> ! <UNK> at information <s> more sorry knightley harriet fairfax saw situation which ; having . state she knew </s>`\n",
        "\n",
        "**Analysis of Grammatical Correctness and Fluency:**\n",
        "\n",
        "*   **Unigram Model:** The sentences generated by the unigram model are largely incoherent and lack grammatical structure. Words are chosen almost independently, based solely on their individual frequency in the corpus. This results in sequences that do not form meaningful phrases or follow syntactic rules. For example, sentences like `<s> deal to he </s>` or `<s> the he that might good seen indeed kind -- compliment ; character woodhouse 's must dark <s> slowly <UNK> </s>` are nonsensical. The probability of a word appearing is independent of the words preceding it.\n",
        "\n",
        "*   **Bigram Model:** In contrast, the sentences generated by the bigram model, while not perfectly grammatically correct or fully coherent, show a significantly higher degree of local fluency and structure. Sequences like `admire depend father`, `comfortable differently houses`, or `admire depend father pass` appear. This is because the bigram model considers the probability of a word given the immediately preceding word (e.g., P(word_i | word_i-1)). This 'local context' allows the model to capture common word pairings and short phrases, leading to more plausible transitions between adjacent words.\n",
        "\n",
        "**Explanation of Differences focusing on 'Local Context':**\n",
        "\n",
        "The fundamental difference lies in how each model leverages **local context**.\n",
        "\n",
        "*   The **Unigram model (N=1)** assigns probability to each word independently, P(word_i). It has no memory of previous words. Therefore, it cannot capture any sequential dependencies or grammatical rules, leading to random word salads that resemble the overall word distribution of the corpus but lack structure.\n",
        "\n",
        "*   The **Bigram model (N=2)**, on the other hand, models the probability of a word given the previous word, P(word_i | word_i-1). This means that the choice of the current word is directly influenced by its immediate predecessor. This 'local context' allows the model to learn common word sequences and grammatical structures like `(article, noun)`, `(verb, adverb)`, or common prepositions, leading to more natural and coherent phrasing, even if the overall sentence meaning might still be elusive due to the limited scope of the context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52895dc"
      },
      "source": [
        "## New Corpus Preparation and Training\n",
        "\n",
        "### Subtask:\n",
        "Load a second NLTK corpus, preprocess it, and train a new Smoothed Bigram Model on it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53198db8"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to download the 'brown' NLTK corpus and then load its raw text. Since the 'brown' corpus is typically accessed via `nltk.corpus.brown` and not `gutenberg.raw('brown/cb01.txt')`, I will adjust the loading mechanism to use `brown.raw()` for a more robust approach, specifically using the 'news' category as an example to get a sufficiently large text. The `gutenberg` corpus was used for the first model, and `brown` is a separate corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6010030",
        "outputId": "4f1a07e6-9327-4a8a-f3b3-38e87e82941b"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Download the 'brown' NLTK corpus\n",
        "print(\"Downloading NLTK 'brown' corpus...\")\n",
        "nltk.download('brown', quiet=True)\n",
        "print(\"NLTK 'brown' corpus downloaded.\")\n",
        "\n",
        "# 2. Load the raw text of a suitable file from the 'brown' corpus\n",
        "# Using 'news' category as an example for a substantial text\n",
        "brown_corpus_text = brown.raw(categories='news')\n",
        "print(f\"Loaded text from 'brown' corpus (news category). Length: {len(brown_corpus_text)} characters\")\n",
        "print(f\"\"\"First 500 characters of the brown corpus:\n",
        "{brown_corpus_text[:500]}...\"\"\")\n",
        "\n",
        "# 3. Define a min_frequency for the new corpus\n",
        "min_frequency_brown = 5\n",
        "\n",
        "# 4. Build a vocabulary for the brown_corpus_text\n",
        "brown_vocabulary = build_vocab(brown_corpus_text, min_frequency_brown)\n",
        "print(f\"Brown Corpus Vocabulary size: {len(brown_vocabulary)}\")\n",
        "\n",
        "# 5. Preprocess the brown_corpus_text into sentences\n",
        "brown_prepared_sentences = preprocess_sentences(brown_corpus_text, brown_vocabulary)\n",
        "print(f\"Number of prepared sentences in Brown Corpus: {len(brown_prepared_sentences)}\")\n",
        "\n",
        "# 6. Generate unigram counts for brown_prepared_sentences\n",
        "brown_unigram_counts = Counter()\n",
        "for sentence in brown_prepared_sentences:\n",
        "    for word in sentence:\n",
        "        brown_unigram_counts[(word,)] += 1\n",
        "print(f\"Total unique unigrams in Brown Corpus: {len(brown_unigram_counts)}\")\n",
        "\n",
        "# 7. Generate bigram counts for brown_prepared_sentences\n",
        "brown_bigram_counts = generate_ngrams(brown_prepared_sentences, 2)\n",
        "print(f\"Total unique bigrams in Brown Corpus: {len(brown_bigram_counts)}\")\n",
        "\n",
        "# 8. Get the size of the brown_vocabulary\n",
        "brown_vocabulary_size = len(brown_vocabulary)\n",
        "print(f\"Brown Corpus Vocabulary size for model: {brown_vocabulary_size}\")\n",
        "\n",
        "# 9. Instantiate a new NGramModel for the 'brown' corpus\n",
        "brown_bigram_model_smoothed = NGramModel(brown_bigram_counts, brown_unigram_counts, brown_vocabulary_size)\n",
        "print(\"Brown Corpus Smoothed Bigram Model instantiated.\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'brown' corpus...\n",
            "NLTK 'brown' corpus downloaded.\n",
            "Loaded text from 'brown' corpus (news category). Length: 883606 characters\n",
            "First 500 characters of the brown corpus:\n",
            "\n",
            "\n",
            "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
            "\n",
            "\n",
            "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/c...\n",
            "Brown Corpus Vocabulary size: 2619\n",
            "Number of prepared sentences in Brown Corpus: 4454\n",
            "Total unique unigrams in Brown Corpus: 2619\n",
            "Total unique bigrams in Brown Corpus: 32548\n",
            "Brown Corpus Vocabulary size for model: 2619\n",
            "Brown Corpus Smoothed Bigram Model instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8c3cffd"
      },
      "source": [
        "## Compare Generated Sentences\n",
        "\n",
        "### Subtask:\n",
        "Generate 3 sample sentences from the new Smoothed Bigram Model (trained on the 'brown' corpus) and compare them with sentences generated from the initial model (trained on the 'gutenberg' corpus).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39d65e9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires generating 3 random sentences using the `brown_bigram_model_smoothed` and `brown_vocabulary`. The `generate_sentence` function is already defined in the notebook and can be reused.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1be00c13",
        "outputId": "e329e5d4-3f76-4ab6-ff11-62189d7452f1"
      },
      "source": [
        "print(\"Generating 3 random sentences using the Brown Corpus model:\")\n",
        "for i in range(3):\n",
        "    generated_sentence_brown = generate_sentence(brown_bigram_model_smoothed, brown_vocabulary)\n",
        "    print(f\"Brown Sentence {i+1}: {' '.join(generated_sentence_brown)}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 3 random sentences using the Brown Corpus model:\n",
            "Brown Sentence 1: <s> <UNK> of/in this/dt afternoon/nn pay/vb entered/vbd whether/cs hundreds/nns hear/vb charged/vbn kennedy/np fire/nn recovery/nn nine/cd player/np money/nn cut/vb testified/vbd received/vbn </s>\n",
            "Brown Sentence 2: <s> yesterday/nr jack/np list/nn joseph/np he's/pps+bez task/nn karns/np bounced/vbd puppets/nns nevertheless/rb stevenson/np independence/nn question/nn section/nn texas/np-hl long/jj-tl atomic/jj television/nn federal/jj-tl </s>\n",
            "Brown Sentence 3: <s> the/at university/nn-tl acts/nns traffic/nn around/in unity/nn those/dts my/pp offer/vb january/np eagles/nns-tl thousands/nns daughters/nns of/in the/at farmers/nns authority/nn harvey/np mickey/np </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ab317f"
      },
      "source": [
        "### Comparison of Generated Sentences: Gutenberg vs. Brown Corpus\n",
        "\n",
        "**Gutenberg (Emma) Generated Sentences (from 'Sentence Generator' subtask):**\n",
        "1. `<s> she must pointed animated particulars admire depend father pass belonged able ill. rest invitations serious introducing couple convince catch </s>`\n",
        "2. `<s> be accomplished wanting slow nice goodness where large determination exactly yards small exquisite walk useful madam evil comes hints </s>`\n",
        "3. `<s> emma george suspicions so , you will fresh received earth public reach clear particularly willing important next passage accomplished </s>`\n",
        "4. `<s> ill. table satisfaction immediate allow days support woman ordered admit relief disparity suspect faith unfortunately less height possibility direct </s>`\n",
        "5. `<s> `` rain letter admiration standing between stopping state look thinking at sweet campbell admit drew occupied comfortable differently houses </s>`\n",
        "\n",
        "**Brown Corpus (News) Generated Sentences:**\n",
        "1. `<s> <UNK> of/in this/dt afternoon/nn pay/vb entered/vbd whether/cs hundreds/nns hear/vb charged/vbn kennedy/np fire/nn recovery/nn nine/cd player/np money/nn cut/vb testified/vbd received/vbn </s>`\n",
        "2. `<s> yesterday/nr jack/np list/nn joseph/np he's/pps+bez task/nn karns/np bounced/vbd puppets/nns nevertheless/rb stevenson/np independence/nn question/nn section/nn texas/np-hl long/jj-tl atomic/jj television/nn federal/jj-tl </s>`\n",
        "3. `<s> the/at university/nn-tl acts/nns traffic/nn around/in unity/nn those/dts my/pp offer/vb january/np eagles/nns-tl thousands/nns daughters/nns of/in the/at farmers/nns authority/nn harvey/np mickey/np </s>`\n",
        "\n",
        "**Observations and Comparison:**\n",
        "\n",
        "*   **Grammatical Correctness and Fluency:**\n",
        "    *   **Gutenberg (Emma) Model:** The sentences from the 'Emma' corpus, while not always perfectly coherent, tend to form more recognizably English phrases and sentence fragments. They show some local grammatical correctness, linking articles to nouns, or verbs to adverbs, reflecting the more formal and narrative style of Jane Austen. For instance, phrases like `admire depend father`, `comfortable differently houses`, or `will fresh received earth` exhibit some plausible word pairings, even if the overall sentence structure is broken.\n",
        "    *   **Brown Corpus (News) Model:** The sentences from the 'Brown' news category model appear significantly less coherent and grammatically correct. A striking difference is the presence of part-of-speech tags appended to many words (e.g., `of/in`, `this/dt`, `afternoon/nn`, `pay/vb`). This indicates that the Brown corpus data used for training might have been pre-tagged (like the tagged version of the Brown corpus in NLTK). The model is learning to generate these tagged tokens, which makes the output less readable and grammatically incorrect in a standard sense. If we were to ignore the tags, the word sequences still seem less fluent than the Emma corpus, e.g., `hundreds/nns hear/vb charged/vbn kennedy/np fire/nn`. This might be due to the varied and often journalistic nature of news text, which might have less predictable narrative flow compared to a novel.\n",
        "\n",
        "*   **Style and Tone Reflection:**\n",
        "    *   **Gutenberg (Emma) Model:** The generated sentences often contain words characteristic of a 19th-century novel, such as `admire`, `depend`, `disposition`, `comfortably`, `invitations`. The vocabulary reflects the social and descriptive context of 'Emma', even when jumbled.\n",
        "    *   **Brown Corpus (News) Model:** The generated sentences from the Brown corpus contain words and structures more typical of news articles, such as names (`kennedy/np`, `jack/np`, `joseph/np`), organizations (`university/nn-tl`, `farmers/nns authority/nn`), and action verbs relevant to events (`charged/vbn`, `entered/vbd`, `testified/vbd`, `bounced/vbd`). The presence of explicit part-of-speech tags also gives it a highly technical or linguistic analysis feel, which is different from plain text generation. This suggests the training data was pre-processed with these tags.\n",
        "\n",
        "In summary, both models, being simple bigram models, struggle with long-range coherence and full grammatical correctness. However, the Gutenberg model produces slightly more natural-sounding sequences of words due to the consistent style of its source text. The Brown corpus model's output is heavily influenced by the pre-tagged nature of its training data, resulting in less readable and less conventional English sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5082b85"
      },
      "source": [
        "## Cross-Domain Perplexity Experiment\n",
        "\n",
        "### Subtask:\n",
        "Calculate the perplexity of the Gutenberg model on the Brown corpus test set, and the Brown model on the Gutenberg corpus test set. Then, explain the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "485bbc12",
        "outputId": "3a6be3a2-b7fa-45ef-9b52-69ee800d215b"
      },
      "source": [
        "import math\n",
        "\n",
        "# 1. Define a test set for the Brown corpus\n",
        "brown_test_sentences = brown_prepared_sentences[:500]\n",
        "print(f\"Number of sentences in Brown test set: {len(brown_test_sentences)}\")\n",
        "print(f\"First Brown test sentence: {brown_test_sentences[0]}\")\n",
        "\n",
        "# 2. Calculate the perplexity of bigram_model_smoothed (trained on Gutenberg) using brown_test_sentences\n",
        "perplexity_gutenberg_on_brown = calculate_perplexity(brown_test_sentences, bigram_model_smoothed)\n",
        "print(f\"Perplexity of Gutenberg model on Brown test set: {perplexity_gutenberg_on_brown:.4f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in Brown test set: 500\n",
            "First Brown test sentence: ['<s>', 'the/at', 'fulton/np-tl', 'county/nn-tl', 'grand/jj-tl', '<UNK>', 'said/vbd', 'friday/nr', 'an/at', 'investigation/nn', 'of/in', '<UNK>', '$', 'recent/jj', 'primary/nn', 'election/nn', 'produced/vbd', '``', '/', '``', 'no/at', 'evidence/nn', '``', '/', \"''\", 'that/cs', 'any/dti', '<UNK>', 'took/vbd', 'place/nn', './', '.', '</s>']\n",
            "Perplexity of Gutenberg model on Brown test set: 2098.1836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a366a1"
      },
      "source": [
        "**Reasoning**:\n",
        "calculate the perplexity of the Brown model on the Gutenberg test set, and then print both cross-domain perplexity values as requested in instructions 3 and 4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78eaa5b2",
        "outputId": "6697c62c-5168-42f2-d3f4-2093610454d8"
      },
      "source": [
        "import math\n",
        "\n",
        "# 3. Calculate the perplexity of brown_bigram_model_smoothed (trained on Brown) using the existing test_sentences (from Gutenberg)\n",
        "# test_sentences was already defined earlier in the notebook and contains preprocessed sentences from the Gutenberg corpus.\n",
        "perplexity_brown_on_gutenberg = calculate_perplexity(test_sentences, brown_bigram_model_smoothed)\n",
        "\n",
        "# 4. Print both calculated perplexity values.\n",
        "print(f\"Perplexity of Gutenberg model on Brown test set: {perplexity_gutenberg_on_brown:.4f}\")\n",
        "print(f\"Perplexity of Brown model on Gutenberg test set: {perplexity_brown_on_gutenberg:.4f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Gutenberg model on Brown test set: 2098.1836\n",
            "Perplexity of Brown model on Gutenberg test set: 2458.2630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e287404e"
      },
      "source": [
        "### Explanation of Cross-Domain Perplexity Results\n",
        "\n",
        "**Calculated Perplexities:**\n",
        "*   **Perplexity of Gutenberg model on Brown test set:** 2098.1836\n",
        "*   **Perplexity of Brown model on Gutenberg test set:** 2458.2630\n",
        "\n",
        "**Analysis and Explanation:**\n",
        "\n",
        "Perplexity is a measure of how well a probability distribution or language model predicts a sample. A lower perplexity indicates a better fit of the model to the data. In this cross-domain experiment, we observe very high perplexity scores for both models when tested on a corpus different from their training data:\n",
        "\n",
        "1.  **Gutenberg Model on Brown Test Set (Perplexity: 2098.1836):** The model trained on Jane Austen's 'Emma' (19th-century novel) performs poorly when predicting text from the Brown Corpus 'news' category (20th-century American English news). This is because the vocabulary, style, grammar, and common phraseology of a historical novel are vastly different from modern news articles. The Gutenberg model frequently encounters words and bigrams in the Brown test set that it has never seen or rarely seen in its training data, leading to low probabilities and thus high perplexity.\n",
        "\n",
        "2.  **Brown Model on Gutenberg Test Set (Perplexity: 2458.2630):** Similarly, the model trained on the Brown news corpus struggles significantly when tested on the 'Emma' corpus. The Brown corpus contains many modern terms, names, and grammatical constructions (including the part-of-speech tags as observed in generated sentences) that are absent from 19th-century literature. The 'Emma' test sentences would contain many unknown words or word sequences for the Brown model, resulting in an even higher perplexity score compared to the Gutenberg model on the Brown test set.\n",
        "\n",
        "**Why high perplexity with domain mismatch?**\n",
        "\n",
        "The significantly higher perplexity scores in cross-domain evaluation, compared to same-domain perplexity (e.g., Gutenberg model on its own test set was 159.5785), clearly demonstrate the principle of **domain mismatch**. Language models are highly sensitive to the characteristics of their training data. When a model is applied to text from a different domain, it encounters:\n",
        "\n",
        "*   **Out-of-vocabulary (OOV) words:** Words present in the test set but not in the training corpus's vocabulary. Although '<UNK>' tokens help, they still reduce predictive power.\n",
        "*   **Different word frequencies:** Even common words might have different distributions or be used in different contexts.\n",
        "*   **Different bigram (or N-gram) probabilities:** The sequential dependencies (word pairs) learned from one domain do not transfer well to another. For example, a common bigram in 'Emma' like 'young lady' might be rare in a news corpus, and a news bigram like 'federal government' would not appear in 'Emma'.\n",
        "*   **Stylistic and grammatical differences:** As observed with the generated sentences, the structures and common phrases vary greatly between a novel and news articles.\n",
        "\n",
        "In essence, a model trained on one domain has learned the statistical regularities of *that specific domain*. When asked to predict text from a foreign domain, these learned regularities are largely irrelevant or misleading, leading to a much higher uncertainty (higher perplexity) in its predictions. The Brown model's slightly higher perplexity on the Gutenberg test set might also be exacerbated by the presence of POS tags in its training data, making its 'vocabulary' even more distinct from the plain text of 'Emma'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbd7ed48"
      },
      "source": [
        "## Reflection on Sparsity Issues: Trigram vs. Bigram Models\n",
        "\n",
        "**Sparsity** in N-gram language models refers to the problem where many possible N-grams do not appear in the training corpus, leading to zero counts for these N-grams. This is a significant challenge because a zero count for an N-gram leads to a zero probability (or undefined probability for the history) without smoothing, which means the model cannot predict or evaluate sentences containing these unseen sequences.\n",
        "\n",
        "**Why Trigram Models (N=3) Face Greater Sparsity Issues than Bigram Models (N=2):**\n",
        "\n",
        "The fundamental reason for increased sparsity with higher N-gram orders (like trigrams compared to bigrams) is combinatorial explosion and the nature of natural language.\n",
        "\n",
        "1.  **Increased Number of Possible N-grams:**\n",
        "    *   For a vocabulary size `V`, there are `V^2` possible bigrams (sequences of 2 words). If we consider `<s>` and `</s>` tokens, it's roughly `V^N`. For example, with a vocabulary of 2,000 words, there are 4,000,000 possible bigrams.\n",
        "    *   For trigrams (sequences of 3 words), there are `V^3` possible trigrams. With the same 2,000-word vocabulary, this explodes to 8,000,000,000 possible trigrams.\n",
        "    *   As `N` increases, the number of possible N-grams grows exponentially. Even with a very large corpus, the chances of observing all, or even most, of these possible higher-order N-grams become infinitesimally small.\n",
        "\n",
        "2.  **Longer Context, Fewer Occurrences:**\n",
        "    *   A bigram considers the probability of a word given one preceding word, P(word_i | word_{i-1}). These pairs are relatively common.\n",
        "    *   A trigram considers the probability of a word given two preceding words, P(word_i | word_{i-2} word_{i-1}). This longer context makes the specific sequence `(word_{i-2} word_{i-1} word_i)` much rarer. For example, while \"eat apple\" might be common, \"I eat green apple\" is less common than \"eat green apple\", and \"today I eat green apple\" is even less so.\n",
        "    *   The more specific the context (i.e., the longer the `N`), the less likely it is that this exact sequence will appear frequently enough in any finite training corpus.\n",
        "\n",
        "3.  **Data Limitation:** Natural language corpora, even very large ones, are finite. As the length of the N-gram increases, the proportion of N-grams that have zero counts in the training data (i.e., were never observed) rises dramatically. This means that a trigram model will encounter many more unseen sequences during testing than a bigram model would, leading to a more severe sparsity problem.\n",
        "\n",
        "**Consequences of Sparsity:**\n",
        "\n",
        "*   **Zero Probabilities:** Without smoothing, unseen N-grams lead to zero probabilities, making the language model unable to assign a probability to any sentence containing such an N-gram, effectively rendering the sentence impossible.\n",
        "*   **Reliance on Smoothing:** Higher-order N-gram models rely heavily on smoothing techniques (like Laplace, Kneser-Ney, etc.) to reallocate probability mass from observed N-grams to unobserved ones. However, even with smoothing, if the sparsity is too extreme, the smoothed probabilities might not be very accurate or discriminative.\n",
        "*   **Computational Cost:** A larger number of N-grams (even sparse ones) increases memory requirements and computational complexity for training and inference.\n",
        "\n",
        "In summary, while trigrams capture more linguistic context and can potentially lead to more fluent text generation, the rapidly increasing sparsity problem often makes them impractical without very large corpora and sophisticated smoothing techniques. Bigrams strike a better balance between capturing local context and managing data sparsity for many tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c348f87"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Sentence Generation by Model Order (N-gram)**:\n",
        "    *   **Unigram Model**: Sentences generated by the Unigram model (e.g., \"<s> deal to he </s>\") were largely incoherent and lacked grammatical structure, as words were chosen independently based on individual frequency without considering any sequential context.\n",
        "    *   **Bigram Model**: Sentences generated by the Bigram model (e.g., \"<s> she must pointed animated particulars admire depend father pass belonged able ill. rest invitations serious introducing couple convince catch </s>\") showed a significantly higher degree of local fluency and structure due to modeling the probability of a word given the immediately preceding word.\n",
        "\n",
        "*   **Sentence Generation Across Different Corpora (Bigram)**:\n",
        "    *   **Gutenberg (Emma) Model**: Sentences generated by the model trained on Jane Austen's \"Emma\" often contained vocabulary and structures characteristic of a 19th-century novel, exhibiting more natural-sounding English phrases and local grammatical correctness.\n",
        "    *   **Brown Corpus (News) Model**: Sentences generated by the model trained on the Brown news corpus (e.g., \"<s> \\<UNK\\> of/in this/dt afternoon/nn pay/vb entered/vbd whether/cs hundreds/nns hear/vb charged/vbn kennedy/np fire/nn recovery/nn nine/cd player/np money/nn cut/vb testified/vbd received/vbn </s>\") were less coherent and grammatically conventional. This was largely due to the training data containing words explicitly tagged with Part-of-Speech (POS) information, which the model learned to generate.\n",
        "\n",
        "*   **Language Model Perplexity**:\n",
        "    *   **Same-Domain Perplexity**: The Gutenberg model achieved a perplexity of 159.5785 when tested on its own domain (Gutenberg corpus).\n",
        "    *   **Cross-Domain Perplexity**:\n",
        "        *   The Gutenberg model, when evaluated on the Brown corpus test set, yielded a perplexity of 2098.1836.\n",
        "        *   The Brown model, when evaluated on the Gutenberg corpus test set, resulted in an even higher perplexity of 2458.2630.\n",
        "    *   The significantly higher cross-domain perplexity scores demonstrate a strong **domain mismatch**, as models perform poorly when predicting text from a corpus different from their training data due to variations in vocabulary, style, grammar, and N-gram frequencies.\n",
        "\n",
        "*   **N-gram Sparsity**:\n",
        "    *   **Trigram (N=3) vs. Bigram (N=2)**: Trigram models face greater sparsity issues compared to Bigram models. This is primarily due to the combinatorial explosion of possible N-grams as `N` increases (e.g., \\(V^3\\) for trigrams versus \\(V^2\\) for bigrams, where \\(V\\) is vocabulary size) and the diminishing likelihood of observing specific longer sequences (longer contexts) in any finite training corpus.\n",
        "    *   **Consequences**: Sparsity leads to zero probabilities for unobserved N-grams, requiring heavy reliance on smoothing techniques and increasing computational costs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8LRk5ZngO9lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}