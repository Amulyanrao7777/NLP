{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amulyanrao7777/NLP/blob/main/lab2_extended_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44360ae",
      "metadata": {
        "id": "c44360ae"
      },
      "source": [
        "# NLP Lab – Session 2\n",
        "\n",
        "Topics:\n",
        "- Edit Distance (Library-based)\n",
        "- Text Preprocessing & Normalization\n",
        "- Tokenization\n",
        "- Lowercasing, Stemming, Lemmatization\n",
        "- Stop-word Removal\n",
        "- NLP Tools: NLTK & spaCy (Basics)\n",
        "\n",
        "This lab follows **Module 1 – Foundations of NLP**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c24cb031",
      "metadata": {
        "id": "c24cb031"
      },
      "source": [
        "## 2. Text Preprocessing & Normalization\n",
        "\n",
        "Text preprocessing prepares raw text into a clean and usable form. It improves model performance and consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2556ea95",
      "metadata": {
        "id": "2556ea95"
      },
      "source": [
        "### Sample Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7edb6ecb",
      "metadata": {
        "id": "7edb6ecb"
      },
      "outputs": [],
      "source": [
        "text = \"Natural Language Processing (NLP) is AMAZING!!! It helps machines understand human language.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58cccad5",
      "metadata": {
        "id": "58cccad5"
      },
      "source": [
        "## 3. Tokenization\n",
        "\n",
        "Tokenization splits text into smaller units (tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab895f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aab895f7",
        "outputId": "5f16055e-e930-4883-d3a8-8a0fc4df7662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NNjPazBqNb3",
        "outputId": "5099fb75-2545-41a0-cc4a-8d538c5e91b8"
      },
      "id": "3NNjPazBqNb3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'AMAZING', '!', '!', '!', 'It', 'helps', 'machines', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369e09d5",
      "metadata": {
        "id": "369e09d5"
      },
      "source": [
        "## 4. Text Normalization – Lowercasing\n",
        "\n",
        "Converts all text to lowercase to avoid treating words differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2733061",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2733061",
        "outputId": "2e0ba79b-9bc3-437e-9220-be214b020f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'amazing', '!', '!', '!', 'it', 'helps', 'machines', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "lower_tokens = [word.lower() for word in tokens]\n",
        "print(lower_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d204336c",
      "metadata": {
        "id": "d204336c"
      },
      "source": [
        "## 5. Stop-word Removal\n",
        "\n",
        "Stop-words are common words (is, the, and) that add little meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07f3275",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c07f3275",
        "outputId": "5c267c25-d8ce-4f81-a3d9-5bea49a84b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'amazing', 'helps', 'machines', 'understand', 'human', 'language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in lower_tokens if word.isalpha() and word not in stop_words]\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187858f0",
      "metadata": {
        "id": "187858f0"
      },
      "source": [
        "## 6. Stemming\n",
        "\n",
        "Stemming reduces words to their root form (may not be a valid word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38161264",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38161264",
        "outputId": "55205923-c139-4fb7-c2ac-0247403f139b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'nlp', 'amaz', 'help', 'machin', 'understand', 'human', 'languag']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a36603",
      "metadata": {
        "id": "b1a36603"
      },
      "source": [
        "## 7. Lemmatization\n",
        "\n",
        "Lemmatization converts words to their dictionary form using vocabulary knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7937a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb7937a9",
        "outputId": "61d1ca53-9427-4ccf-9f2e-59dafda448e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'amazing', 'help', 'machine', 'understand', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a748ede5",
      "metadata": {
        "id": "a748ede5"
      },
      "source": [
        "## 8. spaCy Basics\n",
        "\n",
        "spaCy is an industrial-strength NLP library providing fast tokenization and linguistic features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fddaa428",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fddaa428",
        "outputId": "8e550d58-07e2-41ef-bef8-888c7d132fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9936283",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c9936283",
        "outputId": "d06a70ca-9b74-4358-88c8-bb57f4a6759f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589d65ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "589d65ac",
        "outputId": "449760d6-cbe6-4503-b1d8-60950f4a0d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is AMAZING!!! It helps machines understand human language.\n",
            "\n",
            " -----------------\n",
            "Natural → Natural | Stopword: False\n",
            "Language → Language | Stopword: False\n",
            "Processing → Processing | Stopword: False\n",
            "( → ( | Stopword: False\n",
            "NLP → NLP | Stopword: False\n",
            ") → ) | Stopword: False\n",
            "is → be | Stopword: True\n",
            "AMAZING → amazing | Stopword: False\n",
            "! → ! | Stopword: False\n",
            "! → ! | Stopword: False\n",
            "! → ! | Stopword: False\n",
            "It → it | Stopword: True\n",
            "helps → help | Stopword: False\n",
            "machines → machine | Stopword: False\n",
            "understand → understand | Stopword: False\n",
            "human → human | Stopword: False\n",
            "language → language | Stopword: False\n",
            ". → . | Stopword: False\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "print(doc)\n",
        "print('\\n -----------------')\n",
        "for token in doc:\n",
        "    print(token.text, '→', token.lemma_, '| Stopword:', token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9771c70"
      },
      "source": [
        "### What is `spaCy` doing here?\n",
        "\n",
        "In this notebook, `spaCy` is demonstrating its capabilities by processing the `text` string. Specifically, after loading the `en_core_web_sm` model:\n",
        "\n",
        "1.  **`nlp = spacy.load('en_core_web_sm')`**: Initializes the spaCy processing pipeline using the downloaded English small model.\n",
        "2.  **`doc = nlp(text)`**: This is the core step where spaCy processes the input `text`. It runs the text through all the components of the `en_core_web_sm` pipeline (tokenizer, tagger, parser, ner). The result is a `Doc` object, which is a container for accessing all the processed linguistic annotations.\n",
        "3.  **`for token in doc:`**: We then iterate through each `token` in the `doc` object. Each `token` object in spaCy provides easy access to various linguistic features.\n",
        "4.  **`print(token.text, '→', token.lemma_, '| Stopword:', token.is_stop)`**: This line prints three key pieces of information for each token:\n",
        "    *   **`token.text`**: The original text of the token.\n",
        "    *   **`token.lemma_`**: The base or dictionary form (lemma) of the token, similar to lemmatization in NLTK but often more linguistically accurate as spaCy uses a statistical model.\n",
        "    *   **`token.is_stop`**: A boolean indicating whether the token is identified as a stop-word by spaCy's internal stop-word list. This is similar to NLTK's stop-word removal, but integrated directly into spaCy's processing pipeline.\n",
        "\n",
        "In essence, spaCy provides a unified and efficient way to perform multiple NLP tasks (like tokenization, lemmatization, and stop-word identification) on a given text with a single function call, and it often yields more accurate results than rule-based approaches."
      ],
      "id": "a9771c70"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple Inc. is looking at buying U.K. startup for $1 billion!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"{'Token':<12} {'POS':<6} {'Lemma':<10} {'Entity'}\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<6} {token.lemma_:<10} {token.ent_type_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H5j_t860u2k",
        "outputId": "d60fd7a9-44a2-4fe8-f165-33aa5ec8290d"
      },
      "id": "_H5j_t860u2k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token        POS    Lemma      Entity\n",
            "Apple        PROPN  Apple      ORG\n",
            "Inc.         PROPN  Inc.       ORG\n",
            "is           AUX    be         \n",
            "looking      VERB   look       \n",
            "at           ADP    at         \n",
            "buying       VERB   buy        \n",
            "U.K.         PROPN  U.K.       GPE\n",
            "startup      VERB   startup    \n",
            "for          ADP    for        \n",
            "$            SYM    $          MONEY\n",
            "1            NUM    1          MONEY\n",
            "billion      NUM    billion    MONEY\n",
            "!            PUNCT  !          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12831eb2"
      },
      "source": [
        "## 10. Explanations of NLP Concepts and Libraries"
      ],
      "id": "12831eb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fd8dc26"
      },
      "source": [
        "### NLTK `punkt_tab`\n",
        "\n",
        "`nltk.download('punkt_tab')` downloads the `punkt_tab` tokenizer model for NLTK. The `punkt_tab` tokenizer is a pre-trained model that knows how to split text into sentences and words for various languages. It's particularly good at handling punctuation and contractions. In this notebook, it's used by `word_tokenize` to break down the input `text` into individual words and punctuation marks."
      ],
      "id": "2fd8dc26"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e85453"
      },
      "source": [
        "### NLTK `stopwords`\n",
        "\n",
        "`nltk.download('stopwords')` downloads a collection of common words that are often filtered out of text before processing. These words (like 'the', 'is', 'and') are called stop-words because they usually don't carry significant meaning for NLP tasks and can add noise. Removing them can improve efficiency and performance for many models."
      ],
      "id": "26e85453"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40fde96"
      },
      "source": [
        "### `set(stopwords.words('english'))`\n",
        "\n",
        "This line of code retrieves the English stop-words from the downloaded NLTK `stopwords` corpus and converts them into a `set`. A `set` is used for efficient lookup. When checking if a word is a stop-word (`word not in stop_words`), using a set is much faster than iterating through a list, especially for large lists of stop-words."
      ],
      "id": "b40fde96"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fafb344b"
      },
      "source": [
        "### `PorterStemmer`\n",
        "\n",
        "`PorterStemmer` is an algorithm provided by NLTK for **stemming**. Stemming is a heuristic process that chops off suffixes from words to reduce them to their root form. For example, 'running', 'runs', and 'runner' might all be reduced to 'run'. The key characteristic of stemming is that the resulting 'stem' may not always be a valid word in the dictionary (e.g., 'amaz' from 'amazing'). It's generally faster but less accurate than lemmatization."
      ],
      "id": "fafb344b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592eb9a7"
      },
      "source": [
        "### NLTK `wordnet`\n",
        "\n",
        "`nltk.download('wordnet')` downloads WordNet, which is a large lexical database of English. It groups English words into sets of synonyms called synsets, provides short definitions, and records various semantic relations between these synsets. In NLTK, WordNet is primarily used by the `WordNetLemmatizer` to perform lemmatization, allowing it to find the base or dictionary form of a word."
      ],
      "id": "592eb9a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4a5396"
      },
      "source": [
        "### `en_core_web_sm`\n",
        "\n",
        "`en_core_web_sm` is a small English language model provided by spaCy. When you run `!python -m spacy download en_core_web_sm`, you're downloading this pre-trained model. The 'sm' denotes 'small', meaning it's a lightweight model. It includes pipelines for tasks like tokenization, part-of-speech tagging, lemmatization, dependency parsing, and named entity recognition. It's often the default choice for quick experiments or when resources are limited."
      ],
      "id": "8b4a5396"
    },
    {
      "cell_type": "markdown",
      "id": "cccf49b4",
      "metadata": {
        "id": "cccf49b4"
      },
      "source": [
        "## 9. Summary\n",
        "\n",
        "- Edit Distance helps measure string similarity\n",
        "- Tokenization splits text into words\n",
        "- Normalization improves consistency\n",
        "- Stemming vs Lemmatization\n",
        "- NLTK and spaCy are core NLP tools"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}