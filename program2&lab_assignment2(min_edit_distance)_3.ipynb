{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amulyanrao7777/NLP/blob/main/program2%26lab_assignment2(min_edit_distance)_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZaFe9IRhoij",
        "outputId": "190d70af-8dcc-4f3c-8ada-78484cac011b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Weighted Levenshtein (Sub=2) ---\n",
            "Distance ('kitten' -> 'sitting'): 5\n",
            "Matrix State:\n",
            "[[0 1 2 3 4 5 6 7]\n",
            " [1 2 3 4 5 6 7 8]\n",
            " [2 3 2 3 4 5 6 7]\n",
            " [3 4 3 2 3 4 5 6]\n",
            " [4 5 4 3 2 3 4 5]\n",
            " [5 6 5 4 3 4 5 6]\n",
            " [6 7 6 5 4 5 4 5]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Custom implementation where Substitution Cost = 2\n",
        "def weighted_levenshtein(source, target):\n",
        "    rows = len(source) + 1\n",
        "    cols = len(target) + 1\n",
        "\n",
        "    # Initialize Matrix with zeros\n",
        "    dist = [[0 for x in range(cols)] for x in range(rows)] #list comprehension technique is used here\n",
        "\n",
        "    # Initialize first row (Insertion costs: 0 -> j)\n",
        "    for j in range(1, cols):\n",
        "        dist[0][j] = j\n",
        "\n",
        "    # Initialize first column (Deletion costs: i -> 0)\n",
        "    for i in range(1, rows):\n",
        "        dist[i][0] = i\n",
        "\n",
        "    # Fill the matrix (Dynamic Programming)\n",
        "    for i in range(1, rows):\n",
        "        for j in range(1, cols):\n",
        "            # Calculate Substitution Cost\n",
        "            if source[i-1] == target[j-1]:\n",
        "                cost = 0 # Match\n",
        "            else:\n",
        "                cost = 2 # Substitution Penalty (Weighted) #this(weight/replacement cost/substitution penalty) is a manually assigned value for tasks to make decisions easier.\n",
        "\n",
        "            dist[i][j] = min(\n",
        "                dist[i-1][j] + 1,      # Deletion\n",
        "                dist[i][j-1] + 1,      # Insertion\n",
        "                dist[i-1][j-1] + cost  # Substitution / Match\n",
        "            )\n",
        "\n",
        "    return dist[rows-1][cols-1], dist\n",
        "\n",
        "\n",
        "print(f\"\\n--- Weighted Levenshtein (Sub=2) ---\")\n",
        "\n",
        "# Problem 1: FAST -> CATS\n",
        "source_word = \"kitten\"\n",
        "target_word = \"sitting\"\n",
        "\n",
        "distance, matrix = weighted_levenshtein(source_word, target_word)\n",
        "\n",
        "print(f\"Distance ('{source_word}' -> '{target_word}'): {distance}\")\n",
        "print(\"Matrix State:\")\n",
        "print(np.matrix(matrix))\n",
        "\n",
        "# Example Usage\n",
        "#print(f\"Distance: {weighted_levenshtein('ROSY', 'POSE')}\")\n",
        "#print(f\"Distance: {weighted_levenshtein('KITTEN', 'SITTING')}\")\n",
        "#print(f\"Distance: {weighted_levenshtein('Execute', 'Intuition')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit Distance using Library\n",
        "\n",
        "#### #Instead of manually implementing Levenshtein distance, we use a library function. This is commonly used in spell checkers, DNA matching, and NLP preprocessing."
      ],
      "metadata": {
        "id": "eRtZpn4lo0gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# Standard Levenshtein (NLTK)\n",
        "# ==========================================\n",
        "# Default NLTK behavior: Substitution Cost = 1\n",
        "s1 = \"kitten\"\n",
        "s2 = \"sitting\"\n",
        "dist_nltk = nltk.edit_distance(s1, s2)\n",
        "print(f\"--- Standard NLTK (Sub=1) ---\")\n",
        "print(f\"Distance ('{s1}' -> '{s2}'): {dist_nltk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnyVfjjih-qh",
        "outputId": "269ecb07-85da-453a-ff88-bfd8d9bc3fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Standard NLTK (Sub=1) ---\n",
            "Distance ('kitten' -> 'sitting'): 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate distance with Substitution Cost = 2\n",
        "dist = nltk.edit_distance(\"kitten\", \"sitting\", substitution_cost=2)\n",
        "\n",
        "print(f\"Edit Distance: {dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09s9ukbWnOdU",
        "outputId": "2769df8a-465e-4e4e-c723-b86c663fc1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edit Distance: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lab assignment2"
      ],
      "metadata": {
        "id": "_PCjVPkK_U7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[Question](https://docs.google.com/document/d/1wZGgB_LHBeQnnb9V3cr5SijoNwmbKXt9WmbWBU_vpMY/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "zfkv9U08G8gY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9dc4721"
      },
      "source": [
        "import the necessary libraries and download required NLTK resources and the spaCy English model as specified in the instructions. This will set up the environment for the next tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4027f5d5",
        "outputId": "78a72ef0-79c8-47c9-9e65-47fe08e9fe2b"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"Libraries imported and resources loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and resources loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5416037c"
      },
      "source": [
        "normalize the given text. First, define the text to be processed, and then apply lowercasing, remove punctuation, and clean whitespace.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adb2d464",
        "outputId": "dd3c4a32-c0e4-4dfc-9830-9da49e3f1381"
      },
      "source": [
        "import string\n",
        "\n",
        "# Define the input text for normalization\n",
        "text = \"  This is an example sentence, showcasing N.L.P. normalization!  With some extra spaces.  \"\n",
        "\n",
        "# 1. Lowercasing\n",
        "normalized_text = text.lower()\n",
        "\n",
        "# 2. Removing punctuation\n",
        "normalized_text = normalized_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3. Cleaning whitespace (remove extra spaces and leading/trailing spaces)\n",
        "normalized_text = ' '.join(normalized_text.split())\n",
        "\n",
        "print(f\"Original Text: '{text}'\")\n",
        "print(f\"Normalized Text: '{normalized_text}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: '  This is an example sentence, showcasing N.L.P. normalization!  With some extra spaces.  '\n",
            "Normalized Text: 'this is an example sentence showcasing nlp normalization with some extra spaces'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0f46946"
      },
      "source": [
        "Tokenization is a fundamental step in NLP, breaking down text into smaller units (tokens). This allows for further processing, such as stop word removal, stemming, and lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ce92d9"
      },
      "source": [
        "implement the tokenization of the `normalized_text` using both NLTK's `word_tokenize` and spaCy's NLP pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6a463fe",
        "outputId": "3ba66d81-5b8f-4874-c283-dd1747199a12"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"Libraries imported and resources loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and resources loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9056180e",
        "outputId": "55cc6443-68c5-49d1-c8c7-a432ea1c6a61"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# NLTK Tokenization\n",
        "nltk_tokens = nltk.word_tokenize(normalized_text)\n",
        "\n",
        "# spaCy Tokenization\n",
        "doc = nlp(normalized_text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "\n",
        "print(f\"\\nNLTK Tokens: {nltk_tokens}\")\n",
        "print(f\"spaCy Tokens: {spacy_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK Tokens: ['this', 'is', 'an', 'example', 'sentence', 'showcasing', 'nlp', 'normalization', 'with', 'some', 'extra', 'spaces']\n",
            "spaCy Tokens: ['this', 'is', 'an', 'example', 'sentence', 'showcasing', 'nlp', 'normalization', 'with', 'some', 'extra', 'spaces']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c0d7168"
      },
      "source": [
        "Stop Word Removal\n",
        "\n",
        "Stop words are common words (like 'the', 'is', 'and') that often carry little meaning in NLP tasks and can be removed to reduce noise and improve processing efficiency for many applications such as text classification or information retrieval. Both NLTK and spaCy provide efficient ways to filter these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5446dcf2",
        "outputId": "829a145c-9108-46e1-bca4-e1173535787a"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# NLTK Stop Word Removal\n",
        "nltk_stop_words = set(stopwords.words('english'))\n",
        "nltk_filtered_tokens = [word for word in nltk_tokens if word not in nltk_stop_words]\n",
        "\n",
        "# spaCy Stop Word Removal\n",
        "spacy_filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "print(f\"\\nNLTK Filtered Tokens (Stop Words Removed): {nltk_filtered_tokens}\")\n",
        "print(f\"spaCy Filtered Tokens (Stop Words Removed): {spacy_filtered_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK Filtered Tokens (Stop Words Removed): ['example', 'sentence', 'showcasing', 'nlp', 'normalization', 'extra', 'spaces']\n",
            "spaCy Filtered Tokens (Stop Words Removed): ['example', 'sentence', 'showcasing', 'nlp', 'normalization', 'extra', 'spaces']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f2cb383"
      },
      "source": [
        "Stemming and Lemmatization\n",
        "\n",
        "\n",
        "Stemming and lemmatization are techniques used to reduce inflected words to their base or root form. Stemming uses heuristic rules to chop off suffixes, while lemmatization uses vocabulary and morphological analysis to return the base form (lemma) of a word, which is often more accurate than stemming. These processes are crucial for normalizing text and reducing the vocabulary size, which can improve the performance of NLP models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d413f793",
        "outputId": "50e1f001-7571-454f-97ac-a50e97709a96"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# NLTK Stemming (Porter Stemmer)\n",
        "porter_stemmer = PorterStemmer()\n",
        "nltk_stemmed_tokens = [porter_stemmer.stem(word) for word in nltk_filtered_tokens]\n",
        "\n",
        "print(f\"\\nNLTK Stemmed Tokens (Porter Stemmer): {nltk_stemmed_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK Stemmed Tokens (Porter Stemmer): ['exampl', 'sentenc', 'showcas', 'nlp', 'normal', 'extra', 'space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "436775e2",
        "outputId": "a99241e6-eb75-462c-eead-806fe6e2073e"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK Lemmatization (WordNet Lemmatizer)\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "nltk_lemmatized_tokens = [wordnet_lemmatizer.lemmatize(word) for word in nltk_filtered_tokens]\n",
        "\n",
        "print(f\"\\nNLTK Lemmatized Tokens (WordNet Lemmatizer): {nltk_lemmatized_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK Lemmatized Tokens (WordNet Lemmatizer): ['example', 'sentence', 'showcasing', 'nlp', 'normalization', 'extra', 'space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a9ed78d"
      },
      "source": [
        "apply lemmatization using spaCy on the previously processed document (`doc`) and print the lemmatized tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "695bd0aa",
        "outputId": "09a7faab-bb55-43fc-e4a9-4935e183aed3"
      },
      "source": [
        "spacy_lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "print(f\"spaCy Lemmatized Tokens: {spacy_lemmatized_tokens}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Lemmatized Tokens: ['example', 'sentence', 'showcasing', 'nlp', 'normalization', 'extra', 'space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efc66277"
      },
      "source": [
        "Edit Distance Calculation\n",
        "\n",
        "Identify a misspelled word and its correct form, then calculate the edit distance between them using NLTK's `edit_distance` function with substitution costs of 1 and 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07cc94a3",
        "outputId": "997911ea-b36d-4da1-bfa0-800e11028677"
      },
      "source": [
        "# Identify a misspelled word and its correct form\n",
        "mispelled_word = \"normalisation\"\n",
        "correct_word = \"normalization\"\n",
        "\n",
        "# Calculate edit distance with substitution_cost = 1 (default)\n",
        "distance_sub_1 = nltk.edit_distance(mispelled_word, correct_word, substitution_cost=1)\n",
        "\n",
        "# Calculate edit distance with substitution_cost = 2\n",
        "distance_sub_2 = nltk.edit_distance(mispelled_word, correct_word, substitution_cost=2)\n",
        "\n",
        "print(f\"\\nMispelled Word: '{mispelled_word}'\")\n",
        "print(f\"Correct Word: '{correct_word}'\")\n",
        "print(f\"Edit Distance (substitution_cost=1): {distance_sub_1}\")\n",
        "print(f\"Edit Distance (substitution_cost=2): {distance_sub_2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mispelled Word: 'normalisation'\n",
            "Correct Word: 'normalization'\n",
            "Edit Distance (substitution_cost=1): 1\n",
            "Edit Distance (substitution_cost=2): 2\n"
          ]
        }
      ]
    }
  ]
}